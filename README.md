# Intelligent-codebase-rag-assistant
An AI-powered developer assistant that enables natural language querying over a codebase using Retrieval-Augmented Generation (RAG).

This system allows developers to ask questions about a repository and receive accurate, context-aware answers along with source references.

==> Problem Statement

Understanding large codebases is time-consuming. Developers often struggle to:

Locate relevant functions or modules

Understand relationships between components

Onboard quickly to unfamiliar projects

This project solves that by enabling natural language search over code using embeddings + vector search + LLM reasoning.

## System Flow (How It Works)
GitHub Repository
        â†“
ingest.py
        â†“
splitters.py
        â†“
vectorstore.py (HuggingFace Embeddings + ChromaDB)
        â†“
retriever.py (Similarity Search)
        â†“
rag_pipeline.py (Context + LLM)
        â†“
app.py (Streamlit UI â†’ Answer + Sources)

## Tech Stack

Python

Flask (codebase used for testing)

LangChain

HuggingFace Embeddings

ChromaDB (Vector Store)

LLM (for answer generation)

Streamlit (Interactive UI)

## Key Features

ğŸ” Natural language querying over code

ğŸ“š Context-aware responses using RAG

ğŸ“Œ Source file references included in answers

âš¡ Fast similarity-based retrieval

ğŸ§  Embeddings-based semantic search

ğŸ–¥ï¸ Interactive Streamlit interface

## Project Structure
intelligent-codebase-rag-assistant/
â”‚
â”œâ”€â”€ ingest.py          # Code ingestion from GitHub repo
â”œâ”€â”€ splitters.py       # Text chunking logic
â”œâ”€â”€ vectorstore.py     # Embeddings + ChromaDB storage
â”œâ”€â”€ retriever.py       # Similarity search logic
â”œâ”€â”€ llm.py             # LLM configuration & response generation
â”œâ”€â”€ rag_pipeline.py    # RAG orchestration (Retriever + LLM)
â”œâ”€â”€ app.py             # Streamlit UI
â””â”€â”€ README.md

## File-wise Explanation (You Can Put This in README)
ğŸ”¹ ingest.py

Loads and reads source code files from a GitHub repository

Prepares raw code for processing

ğŸ”¹ splitters.py

Splits large code files into smaller chunks

Ensures optimal token size for embeddings

ğŸ”¹ vectorstore.py

Generates embeddings using HuggingFace models

Stores embeddings inside ChromaDB

Enables persistent vector storage

ğŸ”¹ retriever.py

Performs similarity search on stored vectors

Retrieves most relevant code chunks for a query

ğŸ”¹ llm.py

Configures the Large Language Model

Generates final answer using retrieved context

ğŸ”¹ rag_pipeline.py

Combines Retriever + LLM

Implements full Retrieval-Augmented Generation workflow

ğŸ”¹ app.py

Streamlit-based user interface

Accepts natural language queries

Displays answer along with source references

## How to Run the Project

1ï¸âƒ£ Clone the repository
git clone https://github.com/your-username/intelligent-codebase-rag-assistant.git
cd intelligent-codebase-rag-assistant

2ï¸âƒ£ Create virtual environment
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run ingestion
python ingest.py

5ï¸âƒ£ Launch Streamlit UI
streamlit run app.py

## Example Questions

What is the purpose of blueprints.py?

Does this codebase use FastAPI?

How is JSON serialization handled?

How are HTTP requests simulated in tests?

Which files are critical for request handling?

 ## Impact

Improved developer onboarding speed

Reduced manual code navigation time

Enabled semantic search instead of keyword search

Built a scalable foundation for AI-powered developer tools

## Future Improvements

Add multi-repo support

Implement authentication

Add conversation memory

Optimize chunking strategy

Deploy using Docker / Cloud

ğŸ‘¨â€ğŸ’» Author

Alagarsamy S
AI/ML Engineer | Data Scientist
Specialized in RAG Systems, LLM Applications, and Intelligent AI Solutions
